{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler , PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Perceptron, Ridge\n",
    "from sklearn.feature_selection import SelectFromModel, SequentialFeatureSelector, RFE\n",
    "from sklearn.feature_selection import r_regression, f_regression, mutual_info_regression\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestRegressor, VotingClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from xgboost import XGBRegressor , XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    # Load the CSV files\n",
    "    csv0 = pd.read_csv(\"ids_0.csv\")\n",
    "    csv1 = pd.read_csv(\"ids_1.csv\")\n",
    "    csv2 = pd.read_csv(\"ids_2.csv\")\n",
    "    combined = pd.concat([ csv0 , csv1 , csv2], ignore_index=True)\n",
    "\n",
    "    # Load the JSON files\n",
    "    json3 = pd.read_json(\"ids_3.json\", lines=True)\n",
    "    json4 = pd.read_json(\"ids_4.json\", lines=True)\n",
    "    json7 = pd.read_json(\"ids_7.json\", lines=True)\n",
    "    json9 = pd.read_json(\"ids_9.json\", lines=True)\n",
    "    json10 = pd.read_json(\"ids_10.json\", lines=True)\n",
    "    combined = pd.concat([combined , json3 , json4 , json7 , json9 , json10], ignore_index=True)\n",
    "\n",
    "    # Load the Parquet files\n",
    "    parquet5 = pd.read_parquet(\"ids_5.parquet\")\n",
    "    parquet6 = pd.read_parquet(\"ids_6.parquet\")\n",
    "    parquet8 = pd.read_parquet(\"ids_8.parquet\")\n",
    "    parquet11 = pd.read_parquet(\"ids_11.parquet\")\n",
    "    combined = pd.concat([combined , parquet5 , parquet6 , parquet8 , parquet11], ignore_index=True)\n",
    "    \n",
    "    return combined\n",
    "\n",
    "data = extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform section\n",
    "data = data.dropna()\n",
    "\n",
    "#Load section\n",
    "def loadcsv(data: pd.DataFrame) -> None:\n",
    "    \"\"\"This function loads the argument dataframe into a csv file\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): extracted nd transformed dataframe\n",
    "    \"\"\"\n",
    "    data.to_csv('dataSet.csv', index=False)\n",
    "\n",
    "loadcsv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data section (Load to dataframe)\n",
    "data = pd.read_csv('dataSet.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (45556, 78)\n"
     ]
    }
   ],
   "source": [
    "#Drop duplicate values\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "#Identify shape of the dataset.\n",
    "print(\"Dataset shape: \" , data.shape)\n",
    "\n",
    "#Cleanup Column names / remove duplicate columns\n",
    "data.columns = data.columns.str.strip()\n",
    "data.columns = data.columns.str.replace(\" \" , \"_\")\n",
    "data.columns = data.columns.str.replace(\".1\" , \"\")\n",
    "data = data.loc[:,~data.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of missing data in dataset: 0\n"
     ]
    }
   ],
   "source": [
    "#Identify if dataset has missing data.\n",
    "print(\"No. of missing data in dataset: \" + str(data.isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Heartbleed rows\n",
    "data = data[data.Label != 'Heartbleed']\n",
    "\n",
    "#Create attack DF (y axis)\n",
    "attack = data[['Label']]\n",
    "\n",
    "#Delete 'Label' from original dataset\n",
    "data.drop('Label',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing univariate analysis.\n",
    "#Drop features with variance less than .05 (removing 0 variance)\n",
    "data = data.loc[:, data.var(axis=0) >= 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding data\n",
    "attack['Label'] = attack['Label'].replace('BENIGN', 0, regex=True)\n",
    "attack['Label'] = attack['Label'].replace('DoS Hulk', 1, regex=True)\n",
    "attack['Label'] = attack['Label'].replace('DoS GoldenEye', 1, regex=True)\n",
    "attack['Label'] = attack['Label'].replace('DoS Slowhttptest', 1, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "features = data.to_numpy()\n",
    "label = attack.to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.25, random_state=6969)\n",
    "\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_standard = scaler.fit_transform(X_train)\n",
    "X_test_standard = scaler.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0             1             2             3             4   \\\n",
      "count  3.415800e+04  3.415800e+04  3.415800e+04  3.415800e+04  3.415800e+04   \n",
      "mean  -3.952313e-18  4.014718e-17 -3.432272e-17 -1.102487e-17 -4.888388e-18   \n",
      "std    1.000015e+00  1.000015e+00  1.000015e+00  1.000015e+00  1.000015e+00   \n",
      "min   -1.550852e-01 -1.167838e+00 -5.313520e-01 -3.276095e-01 -1.942267e-01   \n",
      "25%   -1.436581e-01 -9.592760e-01 -2.278738e-01 -1.872283e-01 -1.625095e-01   \n",
      "50%   -1.436581e-01  2.993378e-01 -2.555491e-02  2.334343e-02 -4.004590e-02   \n",
      "75%   -1.436581e-01  1.118189e+00  1.767639e-01  9.353402e-02 -1.581748e-02   \n",
      "max    9.188888e+00  1.622811e+00  1.234901e+02  1.159782e+02  1.608300e+02   \n",
      "\n",
      "                 5             6             7             8             9   \\\n",
      "count  3.415800e+04  3.415800e+04  3.415800e+04  3.415800e+04  3.415800e+04   \n",
      "mean   5.408429e-18 -6.926949e-17  9.620763e-18  8.562999e-16  9.227820e-15   \n",
      "std    1.000015e+00  1.000015e+00  1.000015e+00  1.000015e+00  1.000015e+00   \n",
      "min   -2.840237e-01 -9.767042e-01 -1.117655e-01 -6.469396e-01 -9.877898e-01   \n",
      "25%   -2.838024e-01 -8.351590e-01 -1.117655e-01 -2.971486e-01 -9.877898e-01   \n",
      "50%    1.436854e-01  1.657678e-01 -1.117655e-01 -1.222531e-01  1.269041e-01   \n",
      "75%    1.436854e-01  2.870923e-01 -1.117655e-01  9.996706e-02  3.829658e-01   \n",
      "max    1.398173e+02  2.830293e+01  3.289603e+01  2.321087e+01  3.123426e+01   \n",
      "\n",
      "       ...            50            51            52            53  \\\n",
      "count  ...  3.415800e+04  3.415800e+04  3.415800e+04  3.415800e+04   \n",
      "mean   ...  4.576363e-18  1.930393e-16 -1.934553e-17  4.160330e-18   \n",
      "std    ...  1.000015e+00  1.000015e+00  1.000015e+00  1.000015e+00   \n",
      "min    ... -1.890130e-01 -4.754328e+00 -3.052781e-01 -8.207234e-02   \n",
      "25%    ... -7.728320e-02 -1.477962e+00 -3.052781e-01 -8.207234e-02   \n",
      "50%    ... -7.728320e-02  4.878580e-01 -3.047720e-01 -8.207234e-02   \n",
      "75%    ...  3.444658e-02  4.878580e-01 -3.007882e-01 -8.207234e-02   \n",
      "max    ...  1.367917e+02  2.453678e+00  8.897020e+00  6.151187e+01   \n",
      "\n",
      "                 54            55            56            57            58  \\\n",
      "count  3.415800e+04  3.415800e+04  3.415800e+04  3.415800e+04  3.415800e+04   \n",
      "mean  -7.696610e-18 -1.185694e-17  1.913752e-17  5.829662e-17 -3.286661e-17   \n",
      "std    1.000015e+00  1.000015e+00  1.000015e+00  1.000015e+00  1.000015e+00   \n",
      "min   -3.020844e-01 -2.932500e-01 -1.004018e+00 -2.169577e-01 -1.037307e+00   \n",
      "25%   -3.020844e-01 -2.932500e-01 -1.004018e+00 -2.169577e-01 -1.037307e+00   \n",
      "50%   -3.016135e-01 -2.927406e-01 -5.773778e-01 -2.169577e-01 -3.029574e-01   \n",
      "75%   -2.979071e-01 -2.900372e-01  1.220444e+00 -2.169577e-01  1.204633e+00   \n",
      "max    3.723270e+01  9.072245e+00  1.733782e+00  9.457405e+00  1.707925e+00   \n",
      "\n",
      "                 59  \n",
      "count  3.415800e+04  \n",
      "mean  -2.204975e-17  \n",
      "std    1.000015e+00  \n",
      "min   -9.666755e-01  \n",
      "25%   -9.666755e-01  \n",
      "50%   -7.517771e-01  \n",
      "75%    1.229931e+00  \n",
      "max    1.736840e+00  \n",
      "\n",
      "[8 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "#Describe the data to ensure correct standardization\n",
    "print(pd.DataFrame(X_train_standard).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature selection\n",
    "xgbC = XGBClassifier()\n",
    "xgbC.fit(X_train_standard,y_train)\n",
    "selection = SelectFromModel(xgbC,threshold=.01, prefit=True)\n",
    "X_train_selected = selection.transform(X_train)\n",
    "X_test_selected = selection.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train XGBC Model\n",
      "Accuracy: 0.995\n",
      "Precision: 0.995\n",
      "Recall: 0.999\n",
      "F1-score: 0.997\n",
      "\n",
      "Test XGBC Model\n",
      "Accuracy: 0.995\n",
      "Precision: 0.995\n",
      "Recall: 0.999\n",
      "F1-score: 0.997\n"
     ]
    }
   ],
   "source": [
    "#XGBC MODEL\n",
    "xgbC.fit(X_train_selected , y_train)\n",
    "X_train_selected_pred = xgbC.predict(X_train_selected)\n",
    "X_test_selected_pred = xgbC.predict(X_test_selected)\n",
    "\n",
    "train_acc_perc = accuracy_score(y_train, X_train_selected_pred)\n",
    "train_f1score_perc = f1_score(y_train, X_train_selected_pred)\n",
    "train_precision_perc = precision_score(y_train, X_train_selected_pred)\n",
    "train_recall_perc = recall_score(y_train, X_train_selected_pred)\n",
    "\n",
    "print('Train XGBC Model')\n",
    "print('Accuracy: {:.3f}'.format(train_acc_perc))\n",
    "print('Precision: {:.3f}'.format(train_precision_perc))\n",
    "print('Recall: {:.3f}'.format(train_recall_perc))\n",
    "print('F1-score: {:.3f}'.format(train_f1score_perc))\n",
    "\n",
    "test_acc_perc = accuracy_score(y_test, X_test_selected_pred)\n",
    "test_f1score_perc = f1_score(y_test, X_test_selected_pred)\n",
    "test_precision_perc = precision_score(y_test, X_test_selected_pred)\n",
    "test_recall_perc = recall_score(y_test, X_test_selected_pred)\n",
    "\n",
    "print('\\nTest XGBC Model')\n",
    "print('Accuracy: {:.3f}'.format(test_acc_perc))\n",
    "print('Precision: {:.3f}'.format(test_precision_perc))\n",
    "print('Recall: {:.3f}'.format(test_recall_perc))\n",
    "print('F1-score: {:.3f}'.format(test_f1score_perc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
