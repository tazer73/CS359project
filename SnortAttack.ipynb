{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, f1_score, precision_score, recall_score, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineering Pipeline - Extract, Transform, Load (ETL) \n",
    "3. Data integration (extract) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    # Load the CSV files\n",
    "    csv0 = pd.read_csv(\"ids_0.csv\")\n",
    "    csv1 = pd.read_csv(\"ids_1.csv\")\n",
    "    csv2 = pd.read_csv(\"ids_2.csv\")\n",
    "    # Concatenate CSV files.\n",
    "    combined = pd.concat([ csv0 , csv1 , csv2], ignore_index=True)\n",
    "\n",
    "    # Load the JSON files\n",
    "    json3 = pd.read_json(\"ids_3.json\", lines=True)\n",
    "    json4 = pd.read_json(\"ids_4.json\", lines=True)\n",
    "    json7 = pd.read_json(\"ids_7.json\", lines=True)\n",
    "    json9 = pd.read_json(\"ids_9.json\", lines=True)\n",
    "    json10 = pd.read_json(\"ids_10.json\", lines=True)\n",
    "    # Concatenate JSON files.\n",
    "    combined = pd.concat([combined , json3 , json4 , json7 , json9 , json10], ignore_index=True)\n",
    "\n",
    "    # Load the Parquet files\n",
    "    parquet5 = pd.read_parquet(\"ids_5.parquet\")\n",
    "    parquet6 = pd.read_parquet(\"ids_6.parquet\")\n",
    "    parquet8 = pd.read_parquet(\"ids_8.parquet\")\n",
    "    parquet11 = pd.read_parquet(\"ids_11.parquet\")\n",
    "    # Concatenate Parquet files.\n",
    "    combined = pd.concat([combined , parquet5 , parquet6 , parquet8 , parquet11], ignore_index=True)\n",
    "    \n",
    "    # Return files as a Pandas dataframe\n",
    "    return combined\n",
    "\n",
    "# Use the created function and save the dataframe into 'data'. Contains raw data.\n",
    "data = extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data Transformation (Transform) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No data conversion needs to be done.\n",
    "# Drop missing data. 138 rows dropped.\n",
    "print(data.shape)\n",
    "data = data.dropna()\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Data Storage (Load) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store raw data into a new CSV file called 'dataSet.csv'\n",
    "data.to_csv('dataSet.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Reading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV file is read and stored back onto a dataframe 'data'\n",
    "data = pd.read_csv('dataSet.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify shape of the dataset.\n",
    "print(\"Shape of dataset: \",data.shape)\n",
    "# Identify if datase has missing data. Data was cleared in step 4.\n",
    "print(\"Missing data: \",data.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column statistics. Not standardized.\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate analysis. Showing the frequency which ports appear from a sample of 30 elements.\n",
    "# Modifying the number of samples can alter the graphy wildly.\n",
    "# Random state is used to keep the same samples each time the code is run.\n",
    "sample = data[' Destination Port'].sample(30,random_state=22)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.plot(kind='hist', xlabel='Port Number')\n",
    "# Mostly port 80 in this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Pipeline \n",
    "8. Data Preprocessing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying feature names for easier use.\n",
    "# Replacing empty spaces and dots.\n",
    "print(\"Original: \",data.columns[2])\n",
    "data.columns = data.columns.str.strip()\n",
    "data.columns = data.columns.str.replace(\" \" , \"_\")\n",
    "data.columns = data.columns.str.replace(\".1\" , \"\")\n",
    "print(\"Modified: \",data.columns[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "\n",
    "# Drop duplicate values. 15,434 rows dropped.\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Drop duplicate columns. 1 column dropped.\n",
    "data = data.loc[:,~data.columns.duplicated()].copy()\n",
    "\n",
    "# Remove the rows with a 'Heartbleed' as their 'Label'. 11 rows dropped.\n",
    "data = data[data.Label != 'Heartbleed']\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'attack' dataframe. Will become our 'y' test/train.\n",
    "attack = data[['Label']]\n",
    "print(\"Attack shape: \",attack.shape)\n",
    "\n",
    "# Delete 'Label' from original dataset\n",
    "data.drop('Label',axis=1, inplace=True)\n",
    "print(\"Data shape: \",data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "\n",
    "# Drop features(columns) with variance less than 0.05. 3 features(columns) dropped.\n",
    "# These features(columns) are less significant for our traning model.\n",
    "data = data.loc[:, data.var(axis=0) >= 0.05]\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding data.\n",
    "print(\"Original\")\n",
    "print(attack.sample(5, random_state=2))\n",
    "\n",
    "# Results will be binary. 0 = Benign connection. 1 = DoS attack.\n",
    "attack['Label'] = attack['Label'].replace('BENIGN', 0, regex=True)\n",
    "attack['Label'] = attack['Label'].replace('DoS Hulk', 1, regex=True)\n",
    "attack['Label'] = attack['Label'].replace('DoS GoldenEye', 1, regex=True)\n",
    "attack['Label'] = attack['Label'].replace('DoS Slowhttptest', 1, regex=True)\n",
    "\n",
    "print(\"\\nModified\")\n",
    "print(attack.sample(5, random_state=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-2. MORE EXPLORATORY ANALYSIS\n",
    "# Bivariate analysis. Correlations.\n",
    "\n",
    "# Correlation matrix. Displays how strong are the features linked.\n",
    "(data\n",
    " .corr(method='pearson')\n",
    " .style\n",
    " .background_gradient(cmap='RdBu', vmin=-1, vmax=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation visualization. High correlation. They increase similarly.\n",
    "(data\n",
    " .plot.scatter(x='Flow_Duration', y='Fwd_IAT_Total', alpha=0.6, color='purple')\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation visualization. Low correlation. The increase separately.\n",
    "(data\n",
    " .plot.scatter(x='Flow_Duration', y='Init_Win_bytes_forward', alpha=0.6, color='purple')\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data.\n",
    "\n",
    "# Convert 'data' and 'attack' to numpy arrays.\n",
    "data = data.to_numpy()\n",
    "attack = attack.to_numpy()\n",
    "\n",
    "# Split the data into 4. Xtrain/yTrain and Xtest/ytest\n",
    "# Split is done 75/25 respectively.\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, attack, test_size=0.25, random_state=6969)\n",
    "# X contains our features while y contains our labels.\n",
    "\n",
    "# Flatten the y sets. Causes warning if not done. Assume it affects the results.\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_standard = scaler.fit_transform(X_train)\n",
    "X_test_standard = scaler.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the data to ensure correct standardization.\n",
    "# Mean is closer to 0 and Standard Deviation(std) is closer to 1.\n",
    "print(pd.DataFrame(X_train_standard).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection. XGBoost Classifier is used to select the most important features.\n",
    "# The inputs are our Xtrain (60 columns) and yTrain (1 column).\n",
    "xgbC = XGBClassifier()\n",
    "xgbC.fit(X_train_standard,y_train)\n",
    "xgbC.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easier visualization to know which features are important and their score.\n",
    "pyplot.bar(range(len(xgbC.feature_importances_)), xgbC.feature_importances_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the features that have an importance higher than .01 are kept.\n",
    "\n",
    "# Use SelectFromModel to cut down the features based on the result from xgbC.\n",
    "selection = SelectFromModel(xgbC,threshold=.01, prefit=True)\n",
    "\n",
    "# This brings us from 60 features down to just 5 features.\n",
    "X_train_selected = selection.transform(X_train)\n",
    "print(\"Xtrain_selected shape: \",X_train_selected.shape)\n",
    "\n",
    "# Apply the same change to our Xtest.\n",
    "X_test_selected = selection.transform(X_test)\n",
    "print(\"Xtest_selected shape: \",X_test_selected.shape)\n",
    "\n",
    "# In goes XTrainStandard, out comes XTrainSelected. Same for test.\n",
    "# The 'Select's will be that data that we use to train our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Processed Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our selected Xtrain and Xtest onto their own CSV files. Same as we did in step 5, except we will not load them back this time, we will use them as they are.\n",
    "pd.DataFrame(X_train_selected).to_csv('CleanTrain.csv', index=False)\n",
    "pd.DataFrame(X_test_selected).to_csv('CleanTest.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Model Selection and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before, we used XGBoostClassifier only to select the features that were important and discard the rest.\n",
    "# Now, we will use XGBoostClassifier as a model to train and predict.\n",
    "\n",
    "# XGBC MODEL (Our Best Performing Model)\n",
    "\n",
    "# Training the model\n",
    "xgbC.fit(X_train_selected , y_train)\n",
    "# Make a prediction on the data we trained on.\n",
    "X_train_selected_pred = xgbC.predict(X_train_selected)\n",
    "# Make a prediction on the test data (Didn't train on).\n",
    "X_test_selected_pred = xgbC.predict(X_test_selected)\n",
    "\n",
    "# Scores from the first prediction. (Train data)\n",
    "train_acc_perc = accuracy_score(y_train, X_train_selected_pred)\n",
    "train_f1score_perc = f1_score(y_train, X_train_selected_pred)\n",
    "train_precision_perc = precision_score(y_train, X_train_selected_pred)\n",
    "train_recall_perc = recall_score(y_train, X_train_selected_pred)\n",
    "print('Train XGBC Model')\n",
    "print('Accuracy: {:.3f}'.format(train_acc_perc))\n",
    "print('Precision: {:.3f}'.format(train_precision_perc))\n",
    "print('Recall: {:.3f}'.format(train_recall_perc))\n",
    "print('F1-score: {:.3f}'.format(train_f1score_perc))\n",
    "\n",
    "# Scores from the second prediction. (Test data)\n",
    "test_acc_perc = accuracy_score(y_test, X_test_selected_pred)\n",
    "test_f1score_perc = f1_score(y_test, X_test_selected_pred)\n",
    "test_precision_perc = precision_score(y_test, X_test_selected_pred)\n",
    "test_recall_perc = recall_score(y_test, X_test_selected_pred)\n",
    "print('\\nTest XGBC Model')\n",
    "print('Accuracy: {:.3f}'.format(test_acc_perc))\n",
    "print('Precision: {:.3f}'.format(test_precision_perc))\n",
    "print('Recall: {:.3f}'.format(test_recall_perc))\n",
    "print('F1-score: {:.3f}'.format(test_f1score_perc))\n",
    "\n",
    "# Score calculation methods are imported from sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix allows us to view the results from our prediction easily.\n",
    "#  [Actual No / Predicted No]   [Actual No / Predicted Yes]       Or    [Correct prediction]  [Incorrect prediction]\n",
    "#  [Actual Yes / Predicted No]  [Actual Yes / Predicted Yes]           [Incorrect prediction]  [Correct predicion]\n",
    "\n",
    "# If you add up the numbers we end up with Xtrain + Xtest (The whole dataset before splitting).\n",
    "ConfusionMatrixDisplay.from_estimator(xgbC, np.concatenate((X_train_selected, X_test_selected), axis=0), np.concatenate((y_train, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERCEPTRON MODEL\n",
    "\n",
    "# Training the model\n",
    "perceptron = Perceptron(eta0=0.001, random_state=1)\n",
    "perceptron.fit(X_train_selected,y_train)\n",
    "\n",
    "# Make a prediction on the data we trained on.\n",
    "X_train_selected_pred = perceptron.predict(X_train_selected)\n",
    "# Make a prediction on the test data (Didn't train on).\n",
    "X_test_selected_pred = perceptron.predict(X_test_selected)\n",
    "\n",
    "# Scores from the first prediction. (Train data)\n",
    "train_acc_perc = accuracy_score(y_train, X_train_selected_pred)\n",
    "train_f1score_perc = f1_score(y_train, X_train_selected_pred)\n",
    "train_precision_perc = precision_score(y_train, X_train_selected_pred)\n",
    "train_recall_perc = recall_score(y_train, X_train_selected_pred)\n",
    "print('Train Perceptron Model')\n",
    "print('Accuracy: {:.3f}'.format(train_acc_perc))\n",
    "print('Precision: {:.3f}'.format(train_precision_perc))\n",
    "print('Recall: {:.3f}'.format(train_recall_perc))\n",
    "print('F1-score: {:.3f}'.format(train_f1score_perc))\n",
    "\n",
    "# Scores from the second prediction. (Train data)\n",
    "test_acc_perc = accuracy_score(y_test, X_test_selected_pred)\n",
    "test_f1score_perc = f1_score(y_test, X_test_selected_pred)\n",
    "test_precision_perc = precision_score(y_test, X_test_selected_pred)\n",
    "test_recall_perc = recall_score(y_test, X_test_selected_pred)\n",
    "print('\\nTest Perceptron Model')\n",
    "print('Accuracy: {:.3f}'.format(test_acc_perc))\n",
    "print('Precision: {:.3f}'.format(test_precision_perc))\n",
    "print('Recall: {:.3f}'.format(test_recall_perc))\n",
    "print('F1-score: {:.3f}'.format(test_f1score_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix allows us to view the results from our prediction easily.\n",
    "#  [Actual No / Predicted No]   [Actual No / Predicted Yes]       Or    [Correct prediction]  [Incorrect prediction]\n",
    "#  [Actual Yes / Predicted No]  [Actual Yes / Predicted Yes]           [Incorrect prediction]  [Correct predicion]\n",
    "\n",
    "# If you add up the numbers we end up with Xtrain + Xtest (The whole dataset before splitting).\n",
    "ConfusionMatrixDisplay.from_estimator(perceptron, np.concatenate((X_train_selected, X_test_selected), axis=0), np.concatenate((y_train, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADABOOST MODEL\n",
    "\n",
    "# Training the model. Perceptron model must be done before since this model uses it.\n",
    "adaboost_perc = AdaBoostClassifier(estimator=Perceptron(eta0=0.001, random_state=1), n_estimators=100, learning_rate=0.001, random_state=1, algorithm='SAMME')\n",
    "adaboost_perc.fit(X_train_selected,y_train)\n",
    "\n",
    "# Make a prediction on the data we trained on.\n",
    "X_train_selected_pred = adaboost_perc.predict(X_train_selected)\n",
    "# Make a prediction on the test data (Didn't train on).\n",
    "X_test_selected_pred = adaboost_perc.predict(X_test_selected)\n",
    "\n",
    "# Scores from the first prediction. (Train data)\n",
    "train_acc_perc = accuracy_score(y_train, X_train_selected_pred)\n",
    "train_f1score_perc = f1_score(y_train, X_train_selected_pred)\n",
    "train_precision_perc = precision_score(y_train, X_train_selected_pred)\n",
    "train_recall_perc = recall_score(y_train, X_train_selected_pred)\n",
    "print('Train ADABoost Model')\n",
    "print('Accuracy: {:.3f}'.format(train_acc_perc))\n",
    "print('Precision: {:.3f}'.format(train_precision_perc))\n",
    "print('Recall: {:.3f}'.format(train_recall_perc))\n",
    "print('F1-score: {:.3f}'.format(train_f1score_perc))\n",
    "\n",
    "# Scores from the second prediction. (Train data)\n",
    "test_acc_perc = accuracy_score(y_test, X_test_selected_pred)\n",
    "test_f1score_perc = f1_score(y_test, X_test_selected_pred)\n",
    "test_precision_perc = precision_score(y_test, X_test_selected_pred)\n",
    "test_recall_perc = recall_score(y_test, X_test_selected_pred)\n",
    "print('\\nTest ADABoost Model')\n",
    "print('Accuracy: {:.3f}'.format(test_acc_perc))\n",
    "print('Precision: {:.3f}'.format(test_precision_perc))\n",
    "print('Recall: {:.3f}'.format(test_recall_perc))\n",
    "print('F1-score: {:.3f}'.format(test_f1score_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix allows us to view the results from our prediction easily.\n",
    "#  [Actual No / Predicted No]   [Actual No / Predicted Yes]       Or    [Correct prediction]  [Incorrect prediction]\n",
    "#  [Actual Yes / Predicted No]  [Actual Yes / Predicted Yes]           [Incorrect prediction]  [Correct predicion]\n",
    "\n",
    "# If you add up the numbers we end up with Xtrain + Xtest (The whole dataset before splitting).\n",
    "ConfusionMatrixDisplay.from_estimator(adaboost_perc, np.concatenate((X_train_selected, X_test_selected), axis=0), np.concatenate((y_train, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGISTIC REGRESSION MODEL\n",
    "\n",
    "# Training the model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_selected,y_train)\n",
    "# Make a prediction on the data we trained on.\n",
    "X_train_selected_pred = lr.predict(X_train_selected)\n",
    "# Make a prediction on the test data (Didn't train on).\n",
    "X_test_selected_pred = lr.predict(X_test_selected)\n",
    "\n",
    "# Scores from the first prediction. (Train data)\n",
    "train_acc_perc = accuracy_score(y_train, X_train_selected_pred)\n",
    "train_f1score_perc = f1_score(y_train, X_train_selected_pred)\n",
    "train_precision_perc = precision_score(y_train, X_train_selected_pred)\n",
    "train_recall_perc = recall_score(y_train, X_train_selected_pred)\n",
    "print('Train Logistic Regression Model')\n",
    "print('Accuracy: {:.3f}'.format(train_acc_perc))\n",
    "print('Precision: {:.3f}'.format(train_precision_perc))\n",
    "print('Recall: {:.3f}'.format(train_recall_perc))\n",
    "print('F1-score: {:.3f}'.format(train_f1score_perc))\n",
    "\n",
    "# Scores from the second prediction. (Train data)\n",
    "test_acc_perc = accuracy_score(y_test, X_test_selected_pred)\n",
    "test_f1score_perc = f1_score(y_test, X_test_selected_pred)\n",
    "test_precision_perc = precision_score(y_test, X_test_selected_pred)\n",
    "test_recall_perc = recall_score(y_test, X_test_selected_pred)\n",
    "print('\\nTest Logistic Regression Model')\n",
    "print('Accuracy: {:.3f}'.format(test_acc_perc))\n",
    "print('Precision: {:.3f}'.format(test_precision_perc))\n",
    "print('Recall: {:.3f}'.format(test_recall_perc))\n",
    "print('F1-score: {:.3f}'.format(test_f1score_perc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix allows us to view the results from our prediction easily.\n",
    "#  [Actual No / Predicted No]   [Actual No / Predicted Yes]       Or    [Correct prediction]  [Incorrect prediction]\n",
    "#  [Actual Yes / Predicted No]  [Actual Yes / Predicted Yes]           [Incorrect prediction]  [Correct predicion]\n",
    "\n",
    "# If you add up the numbers we end up with Xtrain + Xtest (The whole dataset before splitting).\n",
    "ConfusionMatrixDisplay.from_estimator(lr, np.concatenate((X_train_selected, X_test_selected), axis=0), np.concatenate((y_train, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAJORITYVOTING MODEL\n",
    "\n",
    "# Training the model. Perceptron, AdaBoost, and LogisticRegression must be done before, since this model uses them.\n",
    "hard_majorityvote = VotingClassifier(estimators=[('perceptron', perceptron),('adaboost_perc', adaboost_perc),('logistic regression', lr)],voting='hard')\n",
    "hard_majorityvote.fit(X_train_selected,y_train)\n",
    "\n",
    "# Make a prediction on the data we trained on.\n",
    "X_train_selected_pred = hard_majorityvote.predict(X_train_selected)\n",
    "# Make a prediction on the test data (Didn't train on).\n",
    "X_test_selected_pred = hard_majorityvote.predict(X_test_selected)\n",
    "\n",
    "# Scores from the first prediction. (Train data)\n",
    "train_acc_perc = accuracy_score(y_train, X_train_selected_pred)\n",
    "train_f1score_perc = f1_score(y_train, X_train_selected_pred)\n",
    "train_precision_perc = precision_score(y_train, X_train_selected_pred)\n",
    "train_recall_perc = recall_score(y_train, X_train_selected_pred)\n",
    "print('Train Mayority Voting Model')\n",
    "print('Accuracy: {:.3f}'.format(train_acc_perc))\n",
    "print('Precision: {:.3f}'.format(train_precision_perc))\n",
    "print('Recall: {:.3f}'.format(train_recall_perc))\n",
    "print('F1-score: {:.3f}'.format(train_f1score_perc))\n",
    "\n",
    "# Scores from the second prediction. (Train data)\n",
    "test_acc_perc = accuracy_score(y_test, X_test_selected_pred)\n",
    "test_f1score_perc = f1_score(y_test, X_test_selected_pred)\n",
    "test_precision_perc = precision_score(y_test, X_test_selected_pred)\n",
    "test_recall_perc = recall_score(y_test, X_test_selected_pred)\n",
    "print('\\nTest Mayority Voting Model')\n",
    "print('Accuracy: {:.3f}'.format(test_acc_perc))\n",
    "print('Precision: {:.3f}'.format(test_precision_perc))\n",
    "print('Recall: {:.3f}'.format(test_recall_perc))\n",
    "print('F1-score: {:.3f}'.format(test_f1score_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix allows us to view the results from our prediction easily.\n",
    "#  [Actual No / Predicted No]   [Actual No / Predicted Yes]       Or    [Correct prediction]  [Incorrect prediction]\n",
    "#  [Actual Yes / Predicted No]  [Actual Yes / Predicted Yes]           [Incorrect prediction]  [Correct predicion]\n",
    "\n",
    "# If you add up the numbers we end up with Xtrain + Xtest (The whole dataset before splitting).\n",
    "ConfusionMatrixDisplay.from_estimator(hard_majorityvote, np.concatenate((X_train_selected, X_test_selected), axis=0), np.concatenate((y_train, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
